# -*- coding: utf-8 -*-
"""VGG19_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MWECOo0nK9MD6fhrxe53ycCPxQBTogWT

Set colab to access the images in Google Drive
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install tensorflow

"""Import the required libraries"""

import os   # To navigate into the drive folders and access the images
import matplotlib.pyplot as plt   # To make plots
from tensorflow import keras
from keras.preprocessing.image import ImageDataGenerator  # To process images in batches and apply transformations
from keras.applications import VGG19   # Import the VGG19 model
from keras.models import Sequential    # Allow us to add another layer to the pretrained model that makes the classification
from keras.layers import Flatten, Dense, Dropout   # Flatten the output layer to make it able for input in the Dense layer
from keras.optimizers import Adam    # Import the Adam optimizer

"""Defining paths to our images"""

# Define the base directory where your 'sample' folder is located
base_dir = '/content/drive/My Drive/Sample'

# Define the paths to the training, validation, and testing directories
train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')
test_dir = os.path.join(base_dir, 'test')

"""## Base model

Data normalization and ImageDataGenerator instances to access the images.
"""

# Rescale the pixel values from [0, 255] to [0, 1] for normalization

train_datagen = ImageDataGenerator(rescale=1./255)

validation_datagen  = ImageDataGenerator(rescale=1./255)
test_datagen  = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224,224),   # This is the input size required for the VGG19 model
    batch_size=32,  # It will be iterating from batches of 20 images
    class_mode= 'binary'  # Label the images in a binary way (Fake or Real)
)

Validation_generator = validation_datagen.flow_from_directory(
    validation_dir,
    target_size=(224,224),   # This is the input size required for the VGG19 model
    batch_size=32,  # It will be iterating from batches of 20 images
    class_mode= 'binary'  # Label the images in a binary way (Fake or Real)
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(224,224),   # This is the input size required for the VGG19 model
    batch_size=20,  # It will be iterating from batches of 20 images
    class_mode= 'binary',  # Label the images in a binary way (Fake or Real)
    shuffle=False)  # No need to shuffle the test data

"""Here we defined the first model, calling VGG19 model with the weights resulted in the training on the imagenet dataset."""

# Load the VGG19 model pre-trained on ImageNet, excluding its top (fully connected) layers
base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers of the base model to prevent them from being updated during training
for layer in base_model.layers:
    layer.trainable = False

# Create a new Sequential model and add the VGG19 base model without dropout
model = Sequential([
    base_model,
    Flatten(),  # Flatten the output of the base model to a 1D vector
    Dense(256, activation='relu'),  # Add a fully connected layer with 256 units and ReLU activation
    Dropout(0.2),  # Add dropout for regularization (reduce overfitting)
    Dense(1, activation='sigmoid')  # Add the output layer with sigmoid activation for binary classification
])

"""Given a binary classification we used binary cross entropy as the loss function and ADAM as the activation function."""

# Model compilation with Adam activation and binary_crossentropy loss function, good for binary classifications
# Metrics also included

model.compile(optimizer=Adam(learning_rate= 1e-4),
               loss='binary_crossentropy',
               metrics=['accuracy'])

"""Now we do the training of our first model on 10 epochs

"""

# Model training
num_epochs = 10

history = model.fit(
    train_generator,
    steps_per_epoch= 40,
    epochs=num_epochs,
    validation_data=Validation_generator,
    validation_steps = 5
)

"""Here there is another try including early stopping and normalization

"""

from keras.callbacks import EarlyStopping
from keras.regularizers import l2

# Load the VGG19 model pre-trained on ImageNet, excluding its top (fully connected) layers
base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers of the base model to prevent them from being updated during training
for layer in base_model.layers:
    layer.trainable = False


model = Sequential([
    base_model,
    Flatten(),  # Flatten the output of the base model to a 1D vector
    Dense(256, activation='relu', kernel_regularizer=l2(0.01)),  # Add a fully connected layer with 256 units and ReLU activation
    Dropout(0.2),  # Add dropout for regularization (reduce overfitting)
    Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01))  # Add the output layer with sigmoid activation for binary classification
])

# Model compilation with Adam activation and binary_crossentropy loss function, good for binary classifications
# Metrics also included

model.compile(optimizer=Adam(learning_rate= 1e-3),
               loss='binary_crossentropy',
               metrics=['accuracy'])

# Model training
num_epochs = 20
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

history = model.fit(
    train_generator,
    steps_per_epoch= 40,   # Batch size is 20, 20 times 40 is 800, the size of the training data
    epochs=num_epochs,
    validation_data=Validation_generator,
    validation_steps = 5,
    callbacks=[early_stopping]
)

"""# Bigger sample including data augmentation (4000 images)

To reduce the overfitting we are runing the model with bigger sample and see the performance, redefining the model to take the sample from the root folders of real and fake images. Early stopping and L2 regularization is going to be used to avoid overfitting. Data normalization and ImageDataGenerator instances to access the images. Data augmentation is applied in the training dataset with rotations, shifts and flips.

"""

# Rescale the pixel values from [0, 255] to [0, 1] for normalization

train_datagen = ImageDataGenerator(rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True)

validation_datagen  = ImageDataGenerator(rescale=1./255)
test_datagen  = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224,224),   # This is the input size required for the VGG19 model
    batch_size=32,  # It will be iterating from batches of 20 images
    class_mode= 'binary'  # Label the images in a binary way (Fake or Real)
)

Validation_generator = validation_datagen.flow_from_directory(
    validation_dir,
    target_size=(224,224),   # This is the input size required for the VGG19 model
    batch_size=32,  # It will be iterating from batches of 20 images
    class_mode= 'binary'  # Label the images in a binary way (Fake or Real)
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(224,224),   # This is the input size required for the VGG19 model
    batch_size=20,  # It will be iterating from batches of 20 images
    class_mode= 'binary',  # Label the images in a binary way (Fake or Real)
    shuffle=False)  # No need to shuffle the test data

"""### Base model, early stopping and dropout. Accuracy 0.87"""

from keras.callbacks import EarlyStopping
from keras.regularizers import l2
from keras.callbacks import ModelCheckpoint


# Load the VGG19 model pre-trained on ImageNet, excluding its top (fully connected) layers
base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers of the base model to prevent them from being updated during training
for layer in base_model.layers:
    layer.trainable = False


model = Sequential([
    base_model,
    Flatten(),  # Flatten the output of the base model to a 1D vector
    Dense(256, activation='relu', kernel_regularizer=l2(0.001)),  # Add a fully connected layer with 256 units and ReLU activation
    Dropout(0.2),  # Add dropout for regularization (reduce overfitting)
    Dense(1, activation='sigmoid', kernel_regularizer=l2(0.001))  # Add the output layer with sigmoid activation for binary classification
])

# Model compilation with Adam activation and binary_crossentropy loss function, good for binary classifications
# Metrics also included

model.compile(optimizer=Adam(learning_rate= 1e-3),
               loss='binary_crossentropy',
               metrics=['accuracy'])

# Model training
num_epochs = 20
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
checkpoint_path = '/content/drive/My Drive/checkpoints/VGG19/e40_5/model_epoch_{epoch:02d}_val_accuracy_{val_accuracy:.4f}.h5'
model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_best_only=False, monitor='val_accuracy', mode='max', verbose=1)


history = model.fit(
    train_generator,
    steps_per_epoch= 40,
    epochs=num_epochs,
    validation_data=Validation_generator,
    validation_steps = 5,
    callbacks=[early_stopping, model_checkpoint]
)

# Plot the training and validation accuracy and loss
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc)+1)

plt.plot(epochs, acc, 'dodgerblue', label='Training accuracy')
plt.plot(epochs, val_acc, 'coral', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'dodgerblue', label='Training loss')
plt.plot(epochs, val_loss, 'coral', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

from keras.models import load_model
from keras.layers.experimental.preprocessing import Rescaling

# Load the saved model
model_path = '/content/drive/My Drive/checkpoints/VGG19/e40_5/model_epoch_05_val_accuracy_0.9000.h5'
model = load_model(model_path)

# Set the path to the test folder
test_dir = '/content/drive/My Drive/Sample/test'

# Define the batch size and image size
batch_size = 32
img_height = 224
img_width = 224

# Load the test data
test_ds = keras.utils.image_dataset_from_directory(
    test_dir,
    seed=123,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    shuffle=False)  # No need to shuffle the test data

# Define the preprocessing layers
rescale_layer = Rescaling(1./255)

# Apply the rescaling layer to the test dataset
test_ds = test_ds.map(lambda x, y: (rescale_layer(x), y))

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(test_ds)
print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')

from sklearn.metrics import confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Get the true labels and predictions
true_labels = test_generator.classes
predictions = model.predict(test_generator)
predicted_labels = np.where(predictions > 0.5, 1, 0).flatten()

# Compute the confusion matrix
conf_matrix = confusion_matrix(true_labels, predicted_labels)

def plot_confusion_matrix(conf_matrix, class_names):
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()

# Plot the confusion matrix
class_names = ['Fake', 'Real']  # Adjust based on your classes
plot_confusion_matrix(conf_matrix, class_names)

"""### Basic model 88 steps per epoch, dropot 0.3. Accuracy 0.85

Now I am setting the steps per epoch to run over the entire dataset, restincting more the training with more dropout and lambda value in regularization
"""

from keras.callbacks import EarlyStopping
from keras.regularizers import l2
from keras.callbacks import ModelCheckpoint


# Load the VGG19 model pre-trained on ImageNet, excluding its top (fully connected) layers
base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers of the base model to prevent them from being updated during training
for layer in base_model.layers:
    layer.trainable = False


model = Sequential([
    base_model,
    Flatten(),  # Flatten the output of the base model to a 1D vector
    Dense(256, activation='relu', kernel_regularizer=l2(0.01)),  # Add a fully connected layer with 256 units and ReLU activation
    Dropout(0.3),  # Add dropout for regularization (reduce overfitting)
    Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01))  # Add the output layer with sigmoid activation for binary classification
])

# Model compilation with Adam activation and binary_crossentropy loss function, good for binary classifications
# Metrics also included

model.compile(optimizer=Adam(learning_rate= 1e-3),
               loss='binary_crossentropy',
               metrics=['accuracy'])

# Model training
num_epochs = 15
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
checkpoint_path = '/content/drive/My Drive/checkpoints/VGG19/e88_26/model_epoch_{epoch:02d}_val_accuracy_{val_accuracy:.4f}.h5'
model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_best_only=False, monitor='val_accuracy', mode='max', verbose=1)


history = model.fit(
    train_generator,
    steps_per_epoch= 88,
    epochs=num_epochs,
    validation_data=Validation_generator,
    validation_steps = 26,
    callbacks=[early_stopping, model_checkpoint]
)

# Plot the training and validation accuracy and loss
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc)+1)

plt.plot(epochs, acc, 'dodgerblue', label='Training accuracy')
plt.plot(epochs, val_acc, 'coral', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'dodgerblue', label='Training loss')
plt.plot(epochs, val_loss, 'coral', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

"""Now we are going to test some models:"""

from keras.models import load_model
from keras.layers.experimental.preprocessing import Rescaling

# Load the saved model
model_path = '/content/drive/My Drive/checkpoints/VGG19/e88_26/model_epoch_06_val_accuracy_0.8367.h5'
model = load_model(model_path)

# Set the path to the test folder
test_dir = '/content/drive/My Drive/Sample/test'

# Define the batch size and image size
batch_size = 32
img_height = 224
img_width = 224

# Load the test data
test_ds = keras.utils.image_dataset_from_directory(
    test_dir,
    seed=123,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    shuffle=False)  # No need to shuffle the test data

# Define the preprocessing layers
rescale_layer = Rescaling(1./255)

# Apply the rescaling layer to the test dataset
test_ds = test_ds.map(lambda x, y: (rescale_layer(x), y))

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(test_ds)
print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')

from sklearn.metrics import confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Get the true labels and predictions
true_labels = test_generator.classes
predictions = model.predict(test_generator)
predicted_labels = np.where(predictions > 0.5, 1, 0).flatten()

# Compute the confusion matrix
conf_matrix = confusion_matrix(true_labels, predicted_labels)

def plot_confusion_matrix(conf_matrix, class_names):
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()

# Plot the confusion matrix
class_names = ['Fake', 'Real']  # Adjust based on your classes
plot_confusion_matrix(conf_matrix, class_names)

"""### Base model dropout 0.2 and regularization 0.01. Acc 0.80

We will try to tune the model to improve the results
"""

from keras.callbacks import EarlyStopping
from keras.regularizers import l2
from keras.callbacks import ModelCheckpoint


# Load the VGG19 model pre-trained on ImageNet, excluding its top (fully connected) layers
base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers of the base model to prevent them from being updated during training
for layer in base_model.layers:
    layer.trainable = False


model = Sequential([
    base_model,
    Flatten(),  # Flatten the output of the base model to a 1D vector
    Dense(256, activation='relu', kernel_regularizer=l2(0.01)),  # Add a fully connected layer with 256 units and ReLU activation
    Dropout(0.2),  # Add dropout for regularization (reduce overfitting)
    Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01))  # Add the output layer with sigmoid activation for binary classification
])

# Model compilation with Adam activation and binary_crossentropy loss function, good for binary classifications
# Metrics also included

model.compile(optimizer=Adam(learning_rate= 1e-4),
               loss='binary_crossentropy',
               metrics=['accuracy'])

# Model training
num_epochs = 15
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
checkpoint_path = '/content/drive/My Drive/checkpoints/VGG19/e88_26/model_epoch_{epoch:02d}_val_accuracy_{val_accuracy:.4f}.h5'
model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_best_only=False, monitor='val_accuracy', mode='max', verbose=1)


history = model.fit(
    train_generator,
    steps_per_epoch= 88,
    epochs=num_epochs,
    validation_data=Validation_generator,
    validation_steps = 26,
    callbacks=[early_stopping, model_checkpoint]
)

from keras.callbacks import EarlyStopping
from keras.regularizers import l2
from keras.callbacks import ModelCheckpoint


# Load the VGG19 model pre-trained on ImageNet, excluding its top (fully connected) layers
base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers of the base model to prevent them from being updated during training
for layer in base_model.layers:
    layer.trainable = False


model = Sequential([
    base_model,
    Flatten(),  # Flatten the output of the base model to a 1D vector
    Dense(256, activation='relu', kernel_regularizer=l2(0.01)),  # Add a fully connected layer with 256 units and ReLU activation
    Dropout(0.2),  # Add dropout for regularization (reduce overfitting)
    Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01))  # Add the output layer with sigmoid activation for binary classification
])

# Model compilation with Adam activation and binary_crossentropy loss function, good for binary classifications
# Metrics also included

model.compile(optimizer=Adam(learning_rate= 1e-4),
               loss='binary_crossentropy',
               metrics=['accuracy'])

# Model training
num_epochs = 15
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
checkpoint_path = '/content/drive/My Drive/checkpoints/VGG19/e88_26/model_epoch_{epoch:02d}_val_accuracy_{val_accuracy:.4f}.h5'
model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_best_only=False, monitor='val_accuracy', mode='max', verbose=1)


history = model.fit(
    train_generator,
    epochs=num_epochs,
    validation_data=Validation_generator,
    callbacks=[early_stopping, model_checkpoint]
)

# Plot the training and validation accuracy and loss
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc)+1)

plt.plot(epochs, acc, 'dodgerblue', label='Training accuracy')
plt.plot(epochs, val_acc, 'coral', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'dodgerblue', label='Training loss')
plt.plot(epochs, val_loss, 'coral', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

from keras.models import load_model
from keras.layers.experimental.preprocessing import Rescaling

# Load the saved model
model_path = '/content/drive/My Drive/checkpoints/VGG19/e88_26/model_epoch_13_val_accuracy_0.8890.h5'
model = load_model(model_path)

# Set the path to the test folder
test_dir = '/content/drive/My Drive/Sample/test'

# Define the batch size and image size
batch_size = 32
img_height = 224
img_width = 224

# Load the test data
test_ds = keras.utils.image_dataset_from_directory(
    test_dir,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    shuffle=False)  # No need to shuffle the test data

class_names = test_ds.class_names
print("Class names:", class_names)

# Define the preprocessing layers
rescale_layer = Rescaling(1./255)

# Apply the rescaling layer to the test dataset
test_ds = test_ds.map(lambda x, y: (rescale_layer(x), y))

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(test_ds)
print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')

import numpy as np
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Make predictions on the test set
y_true = np.concatenate([y for x, y in test_ds], axis=0)
y_pred_probs = model.predict(test_ds)

# Assuming a binary classification task, threshold the predictions at 0.5
y_pred_classes = (y_pred_probs > 0.5).astype(int).flatten()

# Compute the confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Plot the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

"""### Base model with 2 dense layers, no dropout, relu activation. Accuracy 0.79"""

from keras.callbacks import EarlyStopping
from keras.regularizers import l2
from keras.callbacks import ModelCheckpoint


# Load the VGG19 model pre-trained on ImageNet, excluding its top (fully connected) layers
base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers of the base model to prevent them from being updated during training
for layer in base_model.layers:
    layer.trainable = False


model = Sequential([
    base_model,
    Flatten(),  # Flatten the output of the base model to a 1D vector
    Dense(256, activation='relu'),  # Add a fully connected layer with 256 units and ReLU activation
    Dense(256, activation='relu'),  # Add a second fully connected layer with 256 units and ReLU activation
    # Dropout(0.2),  # Add dropout for regularization (reduce overfitting)
    Dense(1, activation='sigmoid')  # Add the output layer with sigmoid activation for binary classification
])

# Model compilation with Adam activation and binary_crossentropy loss function, good for binary classifications
# Metrics also included

model.compile(optimizer=Adam(learning_rate= 1e-3),
               loss='binary_crossentropy',
               metrics=['accuracy'])

# Model training
num_epochs = 20
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
checkpoint_path = '/content/drive/My Drive/checkpoints/VGG19/3/model_epoch_{epoch:02d}_val_accuracy_{val_accuracy:.4f}.h5'
model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_best_only=False, monitor='val_accuracy', mode='max', verbose=1)


history = model.fit(
    train_generator,
    steps_per_epoch= 88,
    epochs=num_epochs,
    validation_data=Validation_generator,
    validation_steps = 26,
    callbacks=[early_stopping, model_checkpoint]
)

# Plot the training and validation accuracy and loss
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc)+1)

plt.plot(epochs, acc, 'dodgerblue', label='Training accuracy')
plt.plot(epochs, val_acc, 'coral', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'dodgerblue', label='Training loss')
plt.plot(epochs, val_loss, 'coral', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

from keras.models import load_model
from keras.layers.experimental.preprocessing import Rescaling

# Load the saved model
model_path = '/content/drive/My Drive/checkpoints/VGG19/3/Relu/model_epoch_20_val_accuracy_0.8217.h5'
model = load_model(model_path)

# Set the path to the test folder
test_dir = '/content/drive/My Drive/Sample/test'

# Define the batch size and image size
batch_size = 32
img_height = 224
img_width = 224

# Load the test data
test_ds = keras.utils.image_dataset_from_directory(
    test_dir,
    seed=123,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    shuffle=False)  # No need to shuffle the test data

# Define the preprocessing layers
rescale_layer = Rescaling(1./255)

# Apply the rescaling layer to the test dataset
test_ds = test_ds.map(lambda x, y: (rescale_layer(x), y))

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(test_ds)
print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')

from sklearn.metrics import confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Get the true labels and predictions
true_labels = test_generator.classes
predictions = model.predict(test_generator)
predicted_labels = np.where(predictions > 0.5, 1, 0).flatten()

# Compute the confusion matrix
conf_matrix = confusion_matrix(true_labels, predicted_labels)

def plot_confusion_matrix(conf_matrix, class_names):
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()

# Plot the confusion matrix
class_names = ['Fake', 'Real']  # Adjust based on your classes
plot_confusion_matrix(conf_matrix, class_names)

"""### Base model with 2 dense layers, no dropout, Leaky Relu activation. Accuracy 0.8"""

from keras.callbacks import EarlyStopping
from keras.regularizers import l2
from keras.callbacks import ModelCheckpoint


# Load the VGG19 model pre-trained on ImageNet, excluding its top (fully connected) layers
base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers of the base model to prevent them from being updated during training
for layer in base_model.layers:
    layer.trainable = False


model = Sequential([
    base_model,
    Flatten(),  # Flatten the output of the base model to a 1D vector
    Dense(256, activation='leaky_relu'),  # Add a fully connected layer with 256 units and ReLU activation
    Dense(256, activation='leaky_relu'),  # Add a second fully connected layer with 256 units and ReLU activation
    # Dropout(0.2),  # Add dropout for regularization (reduce overfitting)
    Dense(1, activation='sigmoid')  # Add the output layer with sigmoid activation for binary classification
])

# Model compilation with Adam activation and binary_crossentropy loss function, good for binary classifications
# Metrics also included

model.compile(optimizer=Adam(learning_rate= 1e-3),
               loss='binary_crossentropy',
               metrics=['accuracy'])

# Model training
num_epochs = 25
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
checkpoint_path = '/content/drive/My Drive/checkpoints/VGG19/3/model_epoch_{epoch:02d}_val_accuracy_{val_accuracy:.4f}.h5'
model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_best_only=False, monitor='val_accuracy', mode='max', verbose=1)


history = model.fit(
    train_generator,
    steps_per_epoch= 88,
    epochs=num_epochs,
    validation_data=Validation_generator,
    validation_steps = 26,
    callbacks=[early_stopping, model_checkpoint]
)

from keras.models import load_model
from keras.layers.experimental.preprocessing import Rescaling

# Load the saved model
model_path = '/content/drive/My Drive/checkpoints/VGG19/3/Leaky Relu/model_epoch_21_val_accuracy_0.8354.h5'
model = load_model(model_path)

# Set the path to the test folder
test_dir = '/content/drive/My Drive/Sample/test'

# Define the batch size and image size
batch_size = 32
img_height = 224
img_width = 224

# Load the test data
test_ds = keras.utils.image_dataset_from_directory(
    test_dir,
    seed=123,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    shuffle=False)  # No need to shuffle the test data

# Define the preprocessing layers
rescale_layer = Rescaling(1./255)

# Apply the rescaling layer to the test dataset
test_ds = test_ds.map(lambda x, y: (rescale_layer(x), y))

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(test_ds)
print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')

"""### Base model with 3 dense layers, no dropout, Leaky Relu activation. Accuracy 0.8


"""

from keras.callbacks import EarlyStopping
from keras.regularizers import l2
from keras.callbacks import ModelCheckpoint


# Load the VGG19 model pre-trained on ImageNet, excluding its top (fully connected) layers
base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers of the base model to prevent them from being updated during training
for layer in base_model.layers:
    layer.trainable = False


model = Sequential([
    base_model,
    Flatten(),  # Flatten the output of the base model to a 1D vector
    Dense(512, activation='leaky_relu'),  # Add a fully connected layer with 256 units and ReLU activation
    Dense(256, activation='leaky_relu'),  # Add a second fully connected layer with 256 units and ReLU activation
    Dense(128, activation='leaky_relu'),  # Add a third fully connected layer with 256 units and ReLU activation
    # Dropout(0.2),  # Add dropout for regularization (reduce overfitting)
    Dense(1, activation='sigmoid')  # Add the output layer with sigmoid activation for binary classification
])

# Model compilation with Adam activation and binary_crossentropy loss function, good for binary classifications
# Metrics also included

model.compile(optimizer=Adam(learning_rate= 1e-3),
               loss='binary_crossentropy',
               metrics=['accuracy'])

# Model training
num_epochs = 25
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
checkpoint_path = '/content/drive/My Drive/checkpoints/VGG19/4/model_epoch_{epoch:02d}_val_accuracy_{val_accuracy:.4f}.h5'
model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_best_only=False, monitor='val_accuracy', mode='max', verbose=1)


history = model.fit(
    train_generator,
    steps_per_epoch= 88,
    epochs=num_epochs,
    validation_data=Validation_generator,
    validation_steps = 26,
    callbacks=[early_stopping, model_checkpoint]
)

from keras.models import load_model
from keras.layers.experimental.preprocessing import Rescaling

# Load the saved model
model_path = '/content/drive/My Drive/checkpoints/VGG19/4/model_epoch_23_val_accuracy_0.8055.h5'
model = load_model(model_path)

# Set the path to the test folder
test_dir = '/content/drive/My Drive/Sample/test'

# Define the batch size and image size
batch_size = 32
img_height = 224
img_width = 224

# Load the test data
test_ds = keras.utils.image_dataset_from_directory(
    test_dir,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    shuffle=False)  # No need to shuffle the test data

class_names = test_ds.class_names
print("Class names:", class_names)

# Define the preprocessing layers
rescale_layer = Rescaling(1./255)

# Apply the rescaling layer to the test dataset
test_ds = test_ds.map(lambda x, y: (rescale_layer(x), y))

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(test_ds)
print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')

import numpy as np
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Make predictions on the test set
y_true = np.concatenate([y for x, y in test_ds], axis=0)
y_pred_probs = model.predict(test_ds)

# Assuming a binary classification task, threshold the predictions at 0.5
y_pred_classes = (y_pred_probs > 0.5).astype(int).flatten()

# Compute the confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Plot the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

"""### Base model with 3 dense layers, no dropout, Relu activation. Accuracy 0.79"""

from keras.callbacks import EarlyStopping
from keras.regularizers import l2
from keras.callbacks import ModelCheckpoint


# Load the VGG19 model pre-trained on ImageNet, excluding its top (fully connected) layers
base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers of the base model to prevent them from being updated during training
for layer in base_model.layers:
    layer.trainable = False


model = Sequential([
    base_model,
    Flatten(),  # Flatten the output of the base model to a 1D vector
    Dense(512, activation='relu'),  # Add a fully connected layer with 256 units and ReLU activation
    Dense(256, activation='relu'),  # Add a second fully connected layer with 256 units and ReLU activation
    Dense(128, activation='relu'),  # Add a third fully connected layer with 256 units and ReLU activation
    # Dropout(0.2),  # Add dropout for regularization (reduce overfitting)
    Dense(1, activation='sigmoid')  # Add the output layer with sigmoid activation for binary classification
])

# Model compilation with Adam activation and binary_crossentropy loss function, good for binary classifications
# Metrics also included

model.compile(optimizer=Adam(learning_rate= 1e-3),
               loss='binary_crossentropy',
               metrics=['accuracy'])

# Model training
num_epochs = 25
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
checkpoint_path = '/content/drive/My Drive/checkpoints/VGG19/5/model_epoch_{epoch:02d}_val_accuracy_{val_accuracy:.4f}.h5'
model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_best_only=False, monitor='val_accuracy', mode='max', verbose=1)


history = model.fit(
    train_generator,
    steps_per_epoch= 88,
    epochs=num_epochs,
    validation_data=Validation_generator,
    validation_steps = 26,
    callbacks=[early_stopping, model_checkpoint]
)

# Plot the training and validation accuracy and loss
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc)+1)

plt.plot(epochs, acc, 'dodgerblue', label='Training accuracy')
plt.plot(epochs, val_acc, 'coral', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'dodgerblue', label='Training loss')
plt.plot(epochs, val_loss, 'coral', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

from keras.models import load_model
from keras.layers.experimental.preprocessing import Rescaling

# Load the saved model
model_path = '/content/drive/My Drive/checkpoints/VGG19/5/model_epoch_21_val_accuracy_0.8317.h5'
model = load_model(model_path)

# Set the path to the test folder
test_dir = '/content/drive/My Drive/Sample/test'

# Define the batch size and image size
batch_size = 32
img_height = 224
img_width = 224

# Load the test data
test_ds = keras.utils.image_dataset_from_directory(
    test_dir,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    shuffle=False)  # No need to shuffle the test data

class_names = test_ds.class_names
print("Class names:", class_names)

# Define the preprocessing layers
rescale_layer = Rescaling(1./255)

# Apply the rescaling layer to the test dataset
test_ds = test_ds.map(lambda x, y: (rescale_layer(x), y))

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(test_ds)
print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')

import numpy as np
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Make predictions on the test set
y_true = np.concatenate([y for x, y in test_ds], axis=0)
y_pred_probs = model.predict(test_ds)

# Assuming a binary classification task, threshold the predictions at 0.5
y_pred_classes = (y_pred_probs > 0.5).astype(int).flatten()

# Compute the confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Plot the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

"""## Xception model, good training, bad testing"""

from keras.callbacks import EarlyStopping
from keras.regularizers import l2
from keras.callbacks import ModelCheckpoint

from keras.applications import Xception


# Load the Xception model pre-trained on ImageNet, excluding its top (fully connected) layers
base_model = Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers of the base model to prevent them from being updated during training
for layer in base_model.layers:
    layer.trainable = False


model = Sequential([
    base_model,
    Flatten(),  # Flatten the output of the base model to a 1D vector
    Dense(256, activation='relu'),  # Add a fully connected layer with 256 units and ReLU activation
    Dropout(0.2),  # Add dropout for regularization (reduce overfitting)
    Dense(1, activation='sigmoid')  # Add the output layer with sigmoid activation for binary classification
])

# Model compilation with Adam activation and binary_crossentropy loss function, good for binary classifications
# Metrics also included

model.compile(optimizer=Adam(learning_rate= 1e-3),
               loss='binary_crossentropy',
               metrics=['accuracy'])

# Model training
num_epochs = 20
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
checkpoint_path = '/content/drive/My Drive/checkpoints/Xception2/4/model_epoch_{epoch:02d}_val_accuracy_{val_accuracy:.4f}.h5'
model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_best_only=False, monitor='val_accuracy', mode='max', verbose=1)


history = model.fit(
    train_generator,
    steps_per_epoch= 88,
    epochs=num_epochs,
    validation_data=Validation_generator,
    validation_steps = 26,
    callbacks=[model_checkpoint]
)

# Plot the training and validation accuracy and loss
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc)+1)

plt.plot(epochs, acc, 'dodgerblue', label='Training accuracy')
plt.plot(epochs, val_acc, 'coral', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'dodgerblue', label='Training loss')
plt.plot(epochs, val_loss, 'coral', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

from keras.models import load_model
from keras.layers.experimental.preprocessing import Rescaling

# Load the saved model
model_path = '/content/drive/My Drive/checkpoints/Xception2/4/model_epoch_18_val_accuracy_0.9352.h5'
model = load_model(model_path)

# Set the path to the test folder
test_dir = '/content/drive/My Drive/Sample/test'

# Define the batch size and image size
batch_size = 32
img_height = 224
img_width = 224

# Load the test data
test_ds = keras.utils.image_dataset_from_directory(
    test_dir,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    shuffle=False)  # No need to shuffle the test data

class_names = test_ds.class_names
print("Class names:", class_names)

# Define the preprocessing layers
rescale_layer = Rescaling(1./255)

# Apply the rescaling layer to the test dataset
test_ds = test_ds.map(lambda x, y: (rescale_layer(x), y))

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(test_ds)
print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')

import numpy as np
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Make predictions on the test set
y_true = np.concatenate([y for x, y in test_ds], axis=0)
y_pred_probs = model.predict(test_ds)

# Assuming a binary classification task, threshold the predictions at 0.5
y_pred_classes = (y_pred_probs > 0.5).astype(int).flatten()

# Compute the confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Plot the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

"""## Xception Model with regularization

"""

from keras.callbacks import EarlyStopping
from keras.regularizers import l2
from keras.callbacks import ModelCheckpoint

from keras.applications import Xception


# Load the Xception model pre-trained on ImageNet, excluding its top (fully connected) layers
base_model = Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers of the base model to prevent them from being updated during training
for layer in base_model.layers:
    layer.trainable = False


model = Sequential([
    base_model,
    Flatten(),  # Flatten the output of the base model to a 1D vector
    Dense(256, activation='relu', kernel_regularizer=l2(0.01)),  # Add a fully connected layer with 256 units and ReLU activation
    Dropout(0.2),  # Add dropout for regularization (reduce overfitting)
    Dense(1, activation='sigmoid', kernel_regularizer=l2(0.001))  # Add the output layer with sigmoid activation for binary classification
])

# Model compilation with Adam activation and binary_crossentropy loss function, good for binary classifications
# Metrics also included

model.compile(optimizer=Adam(learning_rate= 1e-3),
               loss='binary_crossentropy',
               metrics=['accuracy'])

# Model training
num_epochs = 20
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
checkpoint_path = '/content/drive/My Drive/checkpoints/Xception2/5/model_epoch_{epoch:02d}_val_accuracy_{val_accuracy:.4f}.h5'
model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_best_only=False, monitor='val_accuracy', mode='max', verbose=1)


history = model.fit(
    train_generator,
    steps_per_epoch= 88,
    epochs=num_epochs,
    validation_data=Validation_generator,
    validation_steps = 26,
    callbacks=[model_checkpoint]
)

# Plot the training and validation accuracy and loss
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc)+1)

plt.plot(epochs, acc, 'dodgerblue', label='Training accuracy')
plt.plot(epochs, val_acc, 'coral', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'dodgerblue', label='Training loss')
plt.plot(epochs, val_loss, 'coral', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

from keras.models import load_model
from keras.layers.experimental.preprocessing import Rescaling

# Load the saved model
model_path = '/content/drive/My Drive/checkpoints/Xception2/5/model_epoch_10_val_accuracy_0.8603.h5'
model = load_model(model_path)

# Set the path to the test folder
test_dir = '/content/drive/My Drive/Sample/test'

# Define the batch size and image size
batch_size = 32
img_height = 224
img_width = 224

# Load the test data
test_ds = keras.utils.image_dataset_from_directory(
    test_dir,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    shuffle=False)  # No need to shuffle the test data

class_names = test_ds.class_names
print("Class names:", class_names)

# Define the preprocessing layers
rescale_layer = Rescaling(1./255)

# Apply the rescaling layer to the test dataset
test_ds = test_ds.map(lambda x, y: (rescale_layer(x), y))

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(test_ds)
print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')

"""## Xception with more training data"""

from keras.callbacks import EarlyStopping
from keras.regularizers import l2
from keras.callbacks import ModelCheckpoint

from keras.applications import Xception


# Load the Xception model pre-trained on ImageNet, excluding its top (fully connected) layers
base_model = Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers of the base model to prevent them from being updated during training
for layer in base_model.layers:
    layer.trainable = False


model = Sequential([
    base_model,
    Flatten(),  # Flatten the output of the base model to a 1D vector
    Dense(256, activation='relu'),  # Add a fully connected layer with 256 units and ReLU activation
    Dropout(0.2),  # Add dropout for regularization (reduce overfitting)
    Dense(1, activation='sigmoid')  # Add the output layer with sigmoid activation for binary classification
])

# Model compilation with Adam activation and binary_crossentropy loss function, good for binary classifications
# Metrics also included

model.compile(optimizer=Adam(learning_rate= 1e-3),
               loss='binary_crossentropy',
               metrics=['accuracy'])

# Model training
num_epochs = 20
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
checkpoint_path = '/content/drive/My Drive/checkpoints/Xception2/7/model_epoch_{epoch:02d}_val_accuracy_{val_accuracy:.4f}.h5'
model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_best_only=False, monitor='val_accuracy', mode='max', verbose=1)


history = model.fit(
    train_generator,
    epochs=num_epochs,
    validation_data=Validation_generator,
    callbacks=[model_checkpoint]
)

# Plot the training and validation accuracy and loss
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc)+1)

plt.plot(epochs, acc, 'dodgerblue', label='Training accuracy')
plt.plot(epochs, val_acc, 'coral', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'dodgerblue', label='Training loss')
plt.plot(epochs, val_loss, 'coral', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

from keras.models import load_model
from keras.layers.experimental.preprocessing import Rescaling

# Load the saved model
model_path = '/content/drive/My Drive/checkpoints/Xception2/7/model_epoch_16_val_accuracy_0.9302.h5'
model = load_model(model_path)

# Set the path to the test folder
test_dir = '/content/drive/My Drive/Sample/test'

# Define the batch size and image size
batch_size = 32
img_height = 224
img_width = 224

# Load the test data
test_ds = keras.utils.image_dataset_from_directory(
    test_dir,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    shuffle=False)  # No need to shuffle the test data

class_names = test_ds.class_names
print("Class names:", class_names)

# Define the preprocessing layers
rescale_layer = Rescaling(1./255)

# Apply the rescaling layer to the test dataset
test_ds = test_ds.map(lambda x, y: (rescale_layer(x), y))

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(test_ds)
print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')

import numpy as np
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Make predictions on the test set
y_true = np.concatenate([y for x, y in test_ds], axis=0)
y_pred_probs = model.predict(test_ds)

# Assuming a binary classification task, threshold the predictions at 0.5
y_pred_classes = (y_pred_probs > 0.5).astype(int).flatten()

# Compute the confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Plot the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

"""## Xception with more data, no dropout

> Add blockquote

Model is underftting a bit so we will remove dropout
"""

from keras.callbacks import EarlyStopping
from keras.regularizers import l2
from keras.callbacks import ModelCheckpoint

from keras.applications import Xception


# Load the Xception model pre-trained on ImageNet, excluding its top (fully connected) layers
base_model = Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers of the base model to prevent them from being updated during training
for layer in base_model.layers:
    layer.trainable = False


model = Sequential([
    base_model,
    Flatten(),  # Flatten the output of the base model to a 1D vector
    Dense(256, activation='relu'),  # Add a fully connected layer with 256 units and ReLU activation
    # Dropout(0.2),  # Add dropout for regularization (reduce overfitting)
    Dense(1, activation='sigmoid')  # Add the output layer with sigmoid activation for binary classification
])

# Model compilation with Adam activation and binary_crossentropy loss function, good for binary classifications
# Metrics also included

model.compile(optimizer=Adam(learning_rate= 1e-3),
               loss='binary_crossentropy',
               metrics=['accuracy'])

# Model training
num_epochs = 20
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
checkpoint_path = '/content/drive/My Drive/checkpoints/Xception2/7/model_epoch_{epoch:02d}_val_accuracy_{val_accuracy:.4f}.h5'
model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_best_only=False, monitor='val_accuracy', mode='max', verbose=1)


history = model.fit(
    train_generator,
    epochs=num_epochs,
    validation_data=Validation_generator,
    callbacks=[model_checkpoint]
)

from keras.models import load_model
from keras.layers.experimental.preprocessing import Rescaling

# Load the saved model
model_path = '/content/drive/My Drive/checkpoints/Xception2/7/model_epoch_14_val_accuracy_0.9115.h5'
model = load_model(model_path)

# Set the path to the test folder
test_dir = '/content/drive/My Drive/Sample/test'

# Define the batch size and image size
batch_size = 32
img_height = 224
img_width = 224

# Load the test data
test_ds = keras.utils.image_dataset_from_directory(
    test_dir,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    shuffle=False)  # No need to shuffle the test data

class_names = test_ds.class_names
print("Class names:", class_names)

# Define the preprocessing layers
rescale_layer = Rescaling(1./255)

# Apply the rescaling layer to the test dataset
test_ds = test_ds.map(lambda x, y: (rescale_layer(x), y))

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(test_ds)
print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')

import numpy as np
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Make predictions on the test set
y_true = np.concatenate([y for x, y in test_ds], axis=0)
y_pred_probs = model.predict(test_ds)

# Assuming a binary classification task, threshold the predictions at 0.5
y_pred_classes = (y_pred_probs > 0.5).astype(int).flatten()

# Compute the confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Plot the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

"""# LIME verification"""

from keras.models import load_model
from keras.layers.experimental.preprocessing import Rescaling

# Load the saved model
model_path = '/content/drive/My Drive/checkpoints/VGG19/e40_5/model_epoch_05_val_accuracy_0.9000.h5'
model = load_model(model_path)

# Set the path to the test folder
test_dir = '/content/drive/My Drive/Sample/test'

# Define the batch size and image size
batch_size = 32
img_height = 224
img_width = 224

# Load the test data
test_ds = keras.utils.image_dataset_from_directory(
    test_dir,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    shuffle=False)  # No need to shuffle the test data

class_names = test_ds.class_names
print("Class names:", class_names)

# Define the preprocessing layers
rescale_layer = Rescaling(1./255)

# Apply the rescaling layer to the test dataset
test_ds = test_ds.map(lambda x, y: (rescale_layer(x), y))

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(test_ds)
print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')

import numpy as np
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Make predictions on the test set
y_true = np.concatenate([y for x, y in test_ds], axis=0)
y_pred_probs = model.predict(test_ds)

# Assuming a binary classification task, threshold the predictions at 0.5
y_pred_classes = (y_pred_probs > 0.5).astype(int).flatten()

# Compute the confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Plot the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

import os
import numpy as np

# Placeholder: You need to create this list based on your actual directory structure
# For example: ['/path/to/image1.jpg', '/path/to/image2.jpg', ...]
image_paths = [os.path.join(dp, f) for dp, dn, filenames in os.walk(test_dir) for f in filenames if os.path.splitext(f)[1].lower() in ['.jpg', '.jpeg', '.png']]
image_paths.sort()  # Make sure this list is in the same order as your dataset

# Predictions
predictions = model.predict(test_ds)
predicted_classes = predictions.round().astype(int).flatten()

# Assuming you can access the actual labels in the correct order
# If your dataset is not shuffled and you have a binary classification (e.g., 0 and 1)
actual_labels = [label for _, label in test_ds.unbatch().as_numpy_iterator()]

# Lists for FP and FN images
false_positives = []
false_negatives = []

# Compare predictions with actual labels
for i, (predicted, actual) in enumerate(zip(predicted_classes, actual_labels)):
    if predicted != actual:  # Misclassification
        if predicted < 0.5:  # False positive (predicted Fake, actual Real)
            false_positives.append(image_paths[i])
        else:  # False negative (predicted Real, actual Fake)
            false_negatives.append(image_paths[i])


print("False Negatives:", false_negatives)
print("False Positives:", false_positives)

print( len(false_negatives))
print(len(false_positives))

!pip install lime

from keras.applications.xception import preprocess_input, decode_predictions
from keras.preprocessing import image
import numpy as np
from keras.models import load_model
import matplotlib.pyplot as plt
import tensorflow as tf
from lime import lime_image
import time
import cv2
from skimage.segmentation import mark_boundaries

# Load your model
model = load_model('/content/drive/My Drive/checkpoints/VGG19/e40_5/model_epoch_05_val_accuracy_0.9000.h5')

def custom_preprocess_input(images):
    return images / 255.0  # Scale pixel values to [0, 1]

# Update the predict_fn to use the custom preprocessing
def predict_fn(images):
    processed_images = custom_preprocess_input(images)
    return model.predict(processed_images)

from keras.preprocessing import image
import numpy as np
from skimage.segmentation import mark_boundaries
import matplotlib.pyplot as plt
from lime import lime_image

# Initialize the LIME Image Explainer
explainer = lime_image.LimeImageExplainer()

# Assuming 'predict_fn' is your model's prediction function and 'model' is your loaded model

def process_single_image(image_path, explainer, predict_fn, model):
    """
    Process a single image, generating an explanation and prediction.

    Parameters:
    - image_path: Path to the image file
    - explainer: LIME Image Explainer instance
    - predict_fn: Prediction function of the model
    - model: Trained model

    Returns:
    - annotated_image: Image annotated with explanation
    - predicted_class: Predicted class of the image ('Real' or 'Fake')
    """
    # Load the image and convert it to a numpy array
    img = image.load_img(image_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = img_array / 255.0

    # Generate an explanation for the image using LIME
    explanation = explainer.explain_instance(
        img_array[0].astype('double'),
        predict_fn,
        top_labels=1,
        hide_color=0,
        num_samples=1000
    )

    # Visualize the explanation
    temp, mask = explanation.get_image_and_mask(
        explanation.top_labels[0],
        positive_only=True,
        num_features=5,
        hide_rest=False
    )
    annotated_image = mark_boundaries(temp, mask)

    # Get the model's prediction
    prediction = model.predict(img_array)
    predicted_class = 'Real' if prediction[0][0] > 0.5 else 'Fake'

    return annotated_image, predicted_class

def process_image_list(image_paths, explainer, predict_fn, model):
    """
    Process a list of images, displaying their explanations and predictions in rows of 4, up to a maximum of 16 images.

    Parameters:
    - image_paths: List of paths to the image files
    - explainer: LIME Image Explainer instance
    - predict_fn: Prediction function of the model
    - model: Trained model
    """
    # Track the number of images processed
    count = 0

    # Prepare a new figure
    plt.figure(figsize=(20, 20))

    # Limit the number of images processed to 16
    for image_path in image_paths[:16]:  # Only process the first 16 images
        annotated_image, predicted_class = process_single_image(image_path, explainer, predict_fn, model)

        # Display the annotated image with the prediction
        count += 1
        plt.subplot(4, 4, count)  # Arrange images in 4x4 grid
        plt.imshow(annotated_image)
        plt.title(f'Class: {predicted_class}')
        plt.axis('off')

        if count == 16:
            break  # Stop after displaying 16 images

    plt.tight_layout()
    plt.show()

#false_negatives
process_image_list(false_negatives, explainer, predict_fn, model)

#false_psitives
process_image_list(false_positives, explainer, predict_fn, model)

from keras.preprocessing import image
import numpy as np

for image_to_explain in false_positives:

  # Load the image and convert it to a numpy array
  img = image.load_img(image_to_explain, target_size=(224, 224))
  img_array = image.img_to_array(img)
  img_array = np.expand_dims(img_array, axis=0)
  img_array = img_array / 255.0  # Rescale the image


  # Get the model's prediction
  prediction = model.predict(img_array)
  predicted_class = 'Real' if prediction[0][0] > 0.5 else 'Fake'
  print(f'Image: {image_to_explain}')
  print(f'Prediction: {prediction[0][0]}')
  print(f'Predicted class: {predicted_class}')

"""# Best model retraining with 600 aditional images

### First model, removed the early stoping to run all the 20 epochs. Accuracy 0.89
"""

from keras.callbacks import EarlyStopping
from keras.regularizers import l2
from keras.callbacks import ModelCheckpoint


# Load the VGG19 model pre-trained on ImageNet, excluding its top (fully connected) layers
base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers of the base model to prevent them from being updated during training
for layer in base_model.layers:
    layer.trainable = False


model = Sequential([
    base_model,
    Flatten(),  # Flatten the output of the base model to a 1D vector
    Dense(256, activation='relu', kernel_regularizer=l2(0.001)),  # Add a fully connected layer with 256 units and ReLU activation
    Dropout(0.2),  # Add dropout for regularization (reduce overfitting)
    Dense(1, activation='sigmoid', kernel_regularizer=l2(0.001))  # Add the output layer with sigmoid activation for binary classification
])

# Model compilation with Adam activation and binary_crossentropy loss function, good for binary classifications
# Metrics also included

model.compile(optimizer=Adam(learning_rate= 1e-3),
               loss='binary_crossentropy',
               metrics=['accuracy'])

# Model training
num_epochs = 20
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
checkpoint_path = '/content/drive/My Drive/checkpoints/VGG19/6/model_epoch_{epoch:02d}_val_accuracy_{val_accuracy:.4f}.h5'
model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_best_only=False, monitor='val_accuracy', mode='max', verbose=1)


history = model.fit(
    train_generator,
    steps_per_epoch= 106,
    epochs=num_epochs,
    validation_data=Validation_generator,
    validation_steps = 26,
    callbacks=[model_checkpoint])

# Plot the training and validation accuracy and loss
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc)+1)

plt.plot(epochs, acc, 'dodgerblue', label='Training accuracy')
plt.plot(epochs, val_acc, 'coral', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'dodgerblue', label='Training loss')
plt.plot(epochs, val_loss, 'coral', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

from keras.models import load_model
from keras.layers.experimental.preprocessing import Rescaling

# Load the saved model
model_path = '/content/drive/My Drive/checkpoints/VGG19/6/model_epoch_12_val_accuracy_0.8990.h5'
model = load_model(model_path)

# Set the path to the test folder
test_dir = '/content/drive/My Drive/Sample/test'

# Define the batch size and image size
batch_size = 32
img_height = 224
img_width = 224

# Load the test data
test_ds = keras.utils.image_dataset_from_directory(
    test_dir,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    shuffle=False)  # No need to shuffle the test data

class_names = test_ds.class_names
print("Class names:", class_names)

# Define the preprocessing layers
rescale_layer = Rescaling(1./255)

# Apply the rescaling layer to the test dataset
test_ds = test_ds.map(lambda x, y: (rescale_layer(x), y))

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(test_ds)
print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')

import numpy as np
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Make predictions on the test set
y_true = np.concatenate([y for x, y in test_ds], axis=0)
y_pred_probs = model.predict(test_ds)

# Assuming a binary classification task, threshold the predictions at 0.5
y_pred_classes = (y_pred_probs > 0.5).astype(int).flatten()

# Compute the confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Plot the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

"""### Second model 0.90 accuracy

Here we added only fake images and don't consider new real images
"""

from keras.callbacks import EarlyStopping
from keras.regularizers import l2
from keras.callbacks import ModelCheckpoint


# Load the VGG19 model pre-trained on ImageNet, excluding its top (fully connected) layers
base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers of the base model to prevent them from being updated during training
for layer in base_model.layers:
    layer.trainable = False


model = Sequential([
    base_model,
    Flatten(),  # Flatten the output of the base model to a 1D vector
    Dense(256, activation='relu', kernel_regularizer=l2(0.001)),  # Add a fully connected layer with 256 units and ReLU activation
    Dropout(0.2),  # Add dropout for regularization (reduce overfitting)
    Dense(1, activation='sigmoid', kernel_regularizer=l2(0.001))  # Add the output layer with sigmoid activation for binary classification
])

# Model compilation with Adam activation and binary_crossentropy loss function, good for binary classifications
# Metrics also included

model.compile(optimizer=Adam(learning_rate= 1e-3),
               loss='binary_crossentropy',
               metrics=['accuracy'])

# Model training
num_epochs = 20
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
checkpoint_path = '/content/drive/My Drive/checkpoints/VGG19/8/model_epoch_{epoch:02d}_val_accuracy_{val_accuracy:.4f}.h5'
model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_best_only=False, monitor='val_accuracy', mode='max', verbose=1)


history = model.fit(
    train_generator,
    epochs=num_epochs,
    validation_data=Validation_generator,
    callbacks=[model_checkpoint])

# Plot the training and validation accuracy and loss
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc)+1)

plt.plot(epochs, acc, 'dodgerblue', label='Training accuracy')
plt.plot(epochs, val_acc, 'coral', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'dodgerblue', label='Training loss')
plt.plot(epochs, val_loss, 'coral', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

from keras.models import load_model
from keras.layers.experimental.preprocessing import Rescaling

# Load the saved model
model_path = '/content/drive/My Drive/checkpoints/VGG19/8/model_epoch_08_val_accuracy_0.8828.h5'
model = load_model(model_path)

# Set the path to the test folder
test_dir = '/content/drive/My Drive/Sample/test'

# Define the batch size and image size
batch_size = 32
img_height = 224
img_width = 224

# Load the test data
test_ds = keras.utils.image_dataset_from_directory(
    test_dir,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    shuffle=False)  # No need to shuffle the test data

class_names = test_ds.class_names
print("Class names:", class_names)

# Define the preprocessing layers
rescale_layer = Rescaling(1./255)

# Apply the rescaling layer to the test dataset
test_ds = test_ds.map(lambda x, y: (rescale_layer(x), y))

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(test_ds)
print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')

import numpy as np
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Make predictions on the test set
y_true = np.concatenate([y for x, y in test_ds], axis=0)
y_pred_probs = model.predict(test_ds)

# Assuming a binary classification task, threshold the predictions at 0.5
y_pred_classes = (y_pred_probs > 0.5).astype(int).flatten()

# Compute the confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Plot the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()